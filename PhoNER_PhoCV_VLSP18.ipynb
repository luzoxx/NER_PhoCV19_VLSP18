{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-22T16:00:15.901895Z","iopub.status.busy":"2024-05-22T16:00:15.901039Z","iopub.status.idle":"2024-05-22T16:00:34.270976Z","shell.execute_reply":"2024-05-22T16:00:34.269720Z","shell.execute_reply.started":"2024-05-22T16:00:15.901850Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n","\u001b[0mCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d2fede19fad9fab4f3f937ef0cd058c6ea29b0897924f3e9a5058dceacc9ac05\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["#tải các thư viện cần thiết\n","!pip install transformer\n","!pip install seqeval"]},{"cell_type":"markdown","metadata":{},"source":["# **Thư viện**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:34.273773Z","iopub.status.busy":"2024-05-22T16:00:34.273443Z","iopub.status.idle":"2024-05-22T16:00:38.737243Z","shell.execute_reply":"2024-05-22T16:00:38.736280Z","shell.execute_reply.started":"2024-05-22T16:00:34.273744Z"},"trusted":true},"outputs":[],"source":["#import các thư viện cần thiết\n","import numpy as np\n","import pandas as pd\n","import torch\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#Embedding\n","from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n","\n","#Modeling\n","from torch.utils.data import DataLoader\n","from torch.optim import SGD, Adam\n","from seqeval.metrics import classification_report\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["Chuyển đổi bộ dữ liệu vlsp_2018"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:38.739242Z","iopub.status.busy":"2024-05-22T16:00:38.738781Z","iopub.status.idle":"2024-05-22T16:00:44.418671Z","shell.execute_reply":"2024-05-22T16:00:44.417888Z","shell.execute_reply.started":"2024-05-22T16:00:38.739209Z"},"trusted":true},"outputs":[],"source":["import re\n","import os\n","train_dir = os.listdir('/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-train-Jan14')\n","test_dir = os.listdir('/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-Test-Domains')\n","val_dir = os.listdir('/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-dev')\n","\n","def load_from_directory(data_dir, path, type: str):\n","  sentences, connl = [], []\n","  for i,n in enumerate(data_dir):\n","    with open(f'{path}/{n}','r',encoding='utf-8') as file:\n","      for line in file:\n","        line = line.strip()\n","        sentences.append(line)\n","\n","  for sentence in sentences:\n","      PER = re.findall(r'<ENAMEX TYPE=\"PERSON\">(.+?)</ENAMEX>', sentence)\n","      ORG = re.findall(r'<ENAMEX TYPE=\"ORGANIZATION\">(.+?)</ENAMEX>', sentence)\n","      LOC = re.findall(r'<ENAMEX TYPE=\"LOCATION\">(.+?)</ENAMEX>', sentence)\n","      MISC = re.findall(r'<ENAMEX TYPE=\"MISCELLANEOUS\">(.+?)</ENAMEX>', sentence)\n","      splitted = re.split('<ENAMEX TYPE=\"PERSON\">|</ENAMEX>|<ENAMEX TYPE=\"ORGANIZATION\">|</ENAMEX>|<ENAMEX TYPE=\"LOCATION\">|</ENAMEX>|<ENAMEX TYPE=\"MISCELLANEOUS\">|</ENAMEX>', sentence)\n","      if PER or ORG or LOC:\n","          for split in splitted:\n","              if split in PER:\n","                  counter = 0\n","                  for token in split.split():\n","                      if counter > 0:\n","                          connl.append(token + ' I-PER')\n","                      else:\n","                          connl.append(token + ' B-PER')\n","                      counter += 1\n","\n","              elif split in ORG:\n","                  counter = 0\n","                  for token in split.split():\n","                      if counter > 0:\n","                          connl.append(token + ' I-ORG')\n","                      else:\n","                          connl.append(token + ' B-ORG')\n","                      counter += 1\n","\n","              elif split in LOC:\n","                  counter = 0\n","                  for token in split.split():\n","                      if counter > 0:\n","                          connl.append(token + ' I-LOC')\n","                      else:\n","                          connl.append(token + ' B-LOC')\n","                      counter += 1\n","\n","              elif split in MISC:\n","                  counter = 0\n","                  for token in split.split():\n","                      if counter > 0:\n","                          connl.append(token + ' I-MISC')\n","                      else:\n","                          connl.append(token + ' B-MISC')\n","                      counter += 1\n","\n","              else:  # Không là các thực thể trên đánh nhãn O\n","                  for token in split.split():\n","                      connl.append(token + ' O')\n","\n","      else:  #Ngoài các thực thể trên sẽ đánh nhãn O\n","          for word in sentence.split():\n","              connl.append(word + ' O')\n","\n","      connl.append('')\n","\n","  if type == 'train':\n","    with open('/kaggle/working/train_set_2018.txt', 'w', encoding='utf-8') as output:\n","        for element in connl:\n","            output.write(element + \"\\n\")\n","  elif type == 'test':\n","    with open('/kaggle/working/test_set_2018.txt', 'w', encoding='utf-8') as output:\n","        for element in connl:\n","            output.write(element + \"\\n\")\n","  else:\n","    with open('/kaggle/working/val_set_2018.txt', 'w', encoding='utf-8') as output:\n","        for element in connl:\n","            output.write(element + \"\\n\")\n","            \n","load_from_directory(train_dir, '/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-train-Jan14','train')\n","load_from_directory(test_dir, '/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-Test-Domains','test')\n","load_from_directory(val_dir, '/kaggle/input/vlsp-2018/VLSP_2018/VLSP2018-NER-dev','val')"]},{"cell_type":"markdown","metadata":{},"source":["func lấy data covidPho"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:44.420081Z","iopub.status.busy":"2024-05-22T16:00:44.419786Z","iopub.status.idle":"2024-05-22T16:00:44.427450Z","shell.execute_reply":"2024-05-22T16:00:44.426636Z","shell.execute_reply.started":"2024-05-22T16:00:44.420058Z"},"trusted":true},"outputs":[],"source":["#hàm đọc dữ liệu\n","def read_dataset(file_path):\n","    tokens=[]\n","    ner_tags=[]\n","    ids=[]\n","    count=1\n","    with open(file_path) as f:\n","        lines=f.readlines()\n","        ts=[]\n","        nts=[]\n","        for line in lines:\n","            line = line.split()\n","            if len(line)==0:\n","                ids.append(count)\n","                tokens.append(ts)\n","                ner_tags.append(nts)\n","                ts=[]\n","                nts=[]\n","                count+=1\n","            else:\n","                ts.append(line[0])\n","                nts.append(line[-1])\n","    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n","    return data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:44.430663Z","iopub.status.busy":"2024-05-22T16:00:44.430323Z","iopub.status.idle":"2024-05-22T16:00:44.821283Z","shell.execute_reply":"2024-05-22T16:00:44.820530Z","shell.execute_reply.started":"2024-05-22T16:00:44.430640Z"},"trusted":true},"outputs":[],"source":["cv_train = read_dataset('/kaggle/input/covid19vi/syllable/train_syllable.conll')\n","cv_val = read_dataset('/kaggle/input/covid19vi/syllable/dev_syllable.conll')\n","cv_test = read_dataset('/kaggle/input/covid19vi/syllable/test_syllable.conll')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:44.823107Z","iopub.status.busy":"2024-05-22T16:00:44.822722Z","iopub.status.idle":"2024-05-22T16:00:44.862684Z","shell.execute_reply":"2024-05-22T16:00:44.861697Z","shell.execute_reply.started":"2024-05-22T16:00:44.823073Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>NER_tags</th>\n","      <th>Tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n","      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>[O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...</td>\n","      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n","      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5022</th>\n","      <td>5023</td>\n","      <td>[O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...</td>\n","      <td>[Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...</td>\n","    </tr>\n","    <tr>\n","      <th>5023</th>\n","      <td>5024</td>\n","      <td>[O, O, O, O, B-DATE, O, O, O, O, O, O, O]</td>\n","      <td>[Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...</td>\n","    </tr>\n","    <tr>\n","      <th>5024</th>\n","      <td>5025</td>\n","      <td>[O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...</td>\n","      <td>[Đây, là, 5, trường, hợp, dương, tính, được, B...</td>\n","    </tr>\n","    <tr>\n","      <th>5025</th>\n","      <td>5026</td>\n","      <td>[O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...</td>\n","      <td>[Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...</td>\n","    </tr>\n","    <tr>\n","      <th>5026</th>\n","      <td>5027</td>\n","      <td>[O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...</td>\n","      <td>[Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5027 rows × 3 columns</p>\n","</div>"],"text/plain":["        Id                                           NER_tags  \\\n","0        1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","1        2  [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...   \n","2        3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","3        4  [O, O, O, O, O, O, O, B-LOCATION, O, B-LOCATIO...   \n","4        5  [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...   \n","...    ...                                                ...   \n","5022  5023  [O, O, O, B-LOCATION, I-LOCATION, I-LOCATION, ...   \n","5023  5024          [O, O, O, O, B-DATE, O, O, O, O, O, O, O]   \n","5024  5025  [O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORG...   \n","5025  5026  [O, O, O, B-DATE, I-DATE, I-DATE, O, B-ORGANIZ...   \n","5026  5027  [O, B-DATE, O, O, O, O, O, O, O, O, O, B-DATE,...   \n","\n","                                                 Tokens  \n","0     [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...  \n","1     [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...  \n","2     [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...  \n","3     [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...  \n","4     [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...  \n","...                                                 ...  \n","5022  [Liên, quan, đến, Bệnh, viện, Bạch, Mai, ,, ôn...  \n","5023  [Mẫu, lần, hai, ngày, 22/7, kết, quả, sàng, lọ...  \n","5024  [Đây, là, 5, trường, hợp, dương, tính, được, B...  \n","5025  [Lúc, 17h, ngày, 7, -, 3, ,, Viện, Vệ, sinh, D...  \n","5026  [Ngày, 12/8, ,, anh, được, cách, ly, tập, trun...  \n","\n","[5027 rows x 3 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["cv_train"]},{"cell_type":"markdown","metadata":{},"source":["Hàm đọc dữ liệu"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:44.864118Z","iopub.status.busy":"2024-05-22T16:00:44.863815Z","iopub.status.idle":"2024-05-22T16:00:44.872271Z","shell.execute_reply":"2024-05-22T16:00:44.871435Z","shell.execute_reply.started":"2024-05-22T16:00:44.864093Z"},"trusted":true},"outputs":[],"source":["def read_dataset(file_path):\n","    tokens=[]\n","    ner_tags=[]\n","    ids=[]\n","    count=1\n","    with open(file_path) as f:\n","        lines=f.readlines()\n","        ts=[]\n","        nts=[]\n","        for line in lines:\n","            line = line.split()\n","            if len(line)==0:\n","                if len(ts) > 0:  # Kiểm tra xem danh sách ts có dữ liệu không trước khi thêm vào\n","                    ids.append(count)\n","                    tokens.append(ts)\n","                    ner_tags.append(nts)\n","                    ts=[]\n","                    nts=[]\n","                    count+=1\n","            else:\n","                ts.append(line[0])\n","                nts.append(line[-1])\n","    if len(ts) > 0:  # Kiểm tra xem danh sách ts có dữ liệu không sau khi kết thúc vòng lặp\n","        ids.append(count)\n","        tokens.append(ts)\n","        ner_tags.append(nts)\n","    data = pd.DataFrame({'Id':ids, 'NER_tags':ner_tags, 'Tokens':tokens})\n","    return data\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:44.874055Z","iopub.status.busy":"2024-05-22T16:00:44.873751Z","iopub.status.idle":"2024-05-22T16:00:45.816875Z","shell.execute_reply":"2024-05-22T16:00:45.816098Z","shell.execute_reply.started":"2024-05-22T16:00:44.874030Z"},"trusted":true},"outputs":[],"source":["vlsp_train = read_dataset('/kaggle/working/train_set_2018.txt')\n","vlsp_val = read_dataset('/kaggle/working/val_set_2018.txt')\n","vlsp_test = read_dataset('/kaggle/working/test_set_2018.txt')"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:45.818265Z","iopub.status.busy":"2024-05-22T16:00:45.817976Z","iopub.status.idle":"2024-05-22T16:00:45.842527Z","shell.execute_reply":"2024-05-22T16:00:45.841579Z","shell.execute_reply.started":"2024-05-22T16:00:45.818242Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>NER_tags</th>\n","      <th>Tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[O, O, O, O, B-ORG, O, O, B-LOC, I-LOC, O, O, ...</td>\n","      <td>[Phung, phí, cơ, hội,, SLNA, và, B., Bình, Dươ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Chơi, phòng, ngự, kín, kẽ, nhưng, hạn, chế, v...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>[B-ORG, I-ORG, O, O, B-PER, I-PER, I-PER, O, B...</td>\n","      <td>[Bình, Dương, :, TM, Bùi, Tấn, Trường, ,, Mich...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>[O, O, B-PER, I-PER, O, B-PER, I-PER, O, B-PER...</td>\n","      <td>[SLNA:, TM, Nguyên, Mạnh, ,, Ngọc, Hải, ,, Văn...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>[O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...</td>\n","      <td>[Những, phút, đầu,, SLNA, nhập, cuộc, khá, thậ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9330</th>\n","      <td>9331</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Lễ, hội, Trung, thu, năm, nay, diễn, ra, tron...</td>\n","    </tr>\n","    <tr>\n","      <th>9331</th>\n","      <td>9332</td>\n","      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...</td>\n","      <td>[Trong, thời, gian, diễn, ra, lễ, hội,, quận, ...</td>\n","    </tr>\n","    <tr>\n","      <th>9332</th>\n","      <td>9333</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Bên, cạnh, đó,, các, hoạt, động, văn, hóa, co...</td>\n","    </tr>\n","    <tr>\n","      <th>9333</th>\n","      <td>9334</td>\n","      <td>[O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...</td>\n","      <td>[Tại, lễ, khai, mạc,, quận, Hoàn, Kiếm, đã, tr...</td>\n","    </tr>\n","    <tr>\n","      <th>9334</th>\n","      <td>9335</td>\n","      <td>[B-PER, I-PER]</td>\n","      <td>[ĐĂNG, ANH]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9335 rows × 3 columns</p>\n","</div>"],"text/plain":["        Id                                           NER_tags  \\\n","0        1  [O, O, O, O, B-ORG, O, O, B-LOC, I-LOC, O, O, ...   \n","1        2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","2        3  [B-ORG, I-ORG, O, O, B-PER, I-PER, I-PER, O, B...   \n","3        4  [O, O, B-PER, I-PER, O, B-PER, I-PER, O, B-PER...   \n","4        5  [O, O, O, B-ORG, O, O, O, O, O, O, O, O, O, O,...   \n","...    ...                                                ...   \n","9330  9331  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","9331  9332  [O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...   \n","9332  9333  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","9333  9334  [O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...   \n","9334  9335                                     [B-PER, I-PER]   \n","\n","                                                 Tokens  \n","0     [Phung, phí, cơ, hội,, SLNA, và, B., Bình, Dươ...  \n","1     [Chơi, phòng, ngự, kín, kẽ, nhưng, hạn, chế, v...  \n","2     [Bình, Dương, :, TM, Bùi, Tấn, Trường, ,, Mich...  \n","3     [SLNA:, TM, Nguyên, Mạnh, ,, Ngọc, Hải, ,, Văn...  \n","4     [Những, phút, đầu,, SLNA, nhập, cuộc, khá, thậ...  \n","...                                                 ...  \n","9330  [Lễ, hội, Trung, thu, năm, nay, diễn, ra, tron...  \n","9331  [Trong, thời, gian, diễn, ra, lễ, hội,, quận, ...  \n","9332  [Bên, cạnh, đó,, các, hoạt, động, văn, hóa, co...  \n","9333  [Tại, lễ, khai, mạc,, quận, Hoàn, Kiếm, đã, tr...  \n","9334                                        [ĐĂNG, ANH]  \n","\n","[9335 rows x 3 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["vlsp_train"]},{"cell_type":"markdown","metadata":{},"source":["hàm đọc nhãn NER"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:45.844061Z","iopub.status.busy":"2024-05-22T16:00:45.843724Z","iopub.status.idle":"2024-05-22T16:00:45.855046Z","shell.execute_reply":"2024-05-22T16:00:45.854189Z","shell.execute_reply.started":"2024-05-22T16:00:45.844032Z"},"trusted":true},"outputs":[],"source":["def get_NER_labels(data):\n","    NERs = list(data['NER_tags'].values)\n","    labels_list = []\n","    for value in NERs:\n","        labels_list = labels_list + value\n","    types = list(set(labels_list))\n","    return types"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:45.856476Z","iopub.status.busy":"2024-05-22T16:00:45.856137Z","iopub.status.idle":"2024-05-22T16:00:48.148579Z","shell.execute_reply":"2024-05-22T16:00:48.147688Z","shell.execute_reply.started":"2024-05-22T16:00:45.856444Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['I-AGE',\n"," 'I-ORGANIZATION',\n"," 'B-GENDER',\n"," 'B-PATIENT_ID',\n"," 'B-TRANSPORTATION',\n"," 'I-SYMPTOM_AND_DISEASE',\n"," 'I-NAME',\n"," 'B-LOCATION',\n"," 'I-TRANSPORTATION',\n"," 'I-PATIENT_ID',\n"," 'B-SYMPTOM_AND_DISEASE',\n"," 'B-JOB',\n"," 'B-ORGANIZATION',\n"," 'I-LOCATION',\n"," 'B-DATE',\n"," 'B-AGE',\n"," 'I-DATE',\n"," 'I-GENDER',\n"," 'I-JOB',\n"," 'O',\n"," 'B-NAME']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["cv_labels = get_NER_labels(cv_train)\n","cv_labels"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:48.149962Z","iopub.status.busy":"2024-05-22T16:00:48.149710Z","iopub.status.idle":"2024-05-22T16:00:48.154145Z","shell.execute_reply":"2024-05-22T16:00:48.153281Z","shell.execute_reply.started":"2024-05-22T16:00:48.149942Z"},"trusted":true},"outputs":[],"source":["translation_dict = {\n","    'I-NAME': 'I-PER',\n","    'B-NAME': 'B-PER',\n","    'I-LOCATION': 'I-LOC',\n","    'B-LOCATION': 'B-LOC',\n","    'I-ORGANIZATION': 'I-ORG',\n","    'B-ORGANIZATION': 'B-ORG'\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:48.155827Z","iopub.status.busy":"2024-05-22T16:00:48.155477Z","iopub.status.idle":"2024-05-22T16:00:48.219410Z","shell.execute_reply":"2024-05-22T16:00:48.218546Z","shell.execute_reply.started":"2024-05-22T16:00:48.155798Z"},"trusted":true},"outputs":[],"source":["def change_labels(row):\n","    new_row = [translation_dict[label] if label in translation_dict else label for label in row]\n","    return new_row\n","cv_train['NER_tags'] = cv_train['NER_tags'].apply(change_labels)\n","cv_val['NER_tags'] = cv_val['NER_tags'].apply(change_labels)\n","cv_test['NER_tags'] = cv_test['NER_tags'].apply(change_labels)\n","# vlsp_train['NER_tags'] = vlsp_train['NER_tags'].apply(change_labels)\n","# vlsp_val['NER_tags'] = vlsp_val['NER_tags'].apply(change_labels)\n","# vlsp_test['NER_tags'] = vlsp_test['NER_tags'].apply(change_labels)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:00:48.224415Z","iopub.status.busy":"2024-05-22T16:00:48.224114Z","iopub.status.idle":"2024-05-22T16:01:01.079037Z","shell.execute_reply":"2024-05-22T16:01:01.078027Z","shell.execute_reply.started":"2024-05-22T16:00:48.224391Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['B-LOC', 'I-LOC', 'I-PER', 'B-ORG', 'I-MISC', 'B-MISC', 'I-ORG', 'O', 'B-PER']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["vlsp_labels = get_NER_labels(vlsp_train)\n","vlsp_labels"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.080595Z","iopub.status.busy":"2024-05-22T16:01:01.080222Z","iopub.status.idle":"2024-05-22T16:01:01.087590Z","shell.execute_reply":"2024-05-22T16:01:01.086562Z","shell.execute_reply.started":"2024-05-22T16:01:01.080564Z"},"trusted":true},"outputs":[],"source":["combined_train = pd.concat([cv_train, vlsp_train], ignore_index=True)\n","combined_train['Id'] = range(1, len(combined_train) + 1)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.088882Z","iopub.status.busy":"2024-05-22T16:01:01.088626Z","iopub.status.idle":"2024-05-22T16:01:01.100188Z","shell.execute_reply":"2024-05-22T16:01:01.099273Z","shell.execute_reply.started":"2024-05-22T16:01:01.088860Z"},"trusted":true},"outputs":[],"source":["combined_val = pd.concat([cv_val, vlsp_val], ignore_index=True)\n","combined_val['Id'] = range(1, len(combined_val) + 1)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.101446Z","iopub.status.busy":"2024-05-22T16:01:01.101194Z","iopub.status.idle":"2024-05-22T16:01:01.112674Z","shell.execute_reply":"2024-05-22T16:01:01.111847Z","shell.execute_reply.started":"2024-05-22T16:01:01.101425Z"},"trusted":true},"outputs":[],"source":["combined_test = pd.concat([cv_test, vlsp_test], ignore_index=True)\n","combined_test['Id'] = range(1, len(combined_test) + 1)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.114044Z","iopub.status.busy":"2024-05-22T16:01:01.113726Z","iopub.status.idle":"2024-05-22T16:01:01.142265Z","shell.execute_reply":"2024-05-22T16:01:01.141405Z","shell.execute_reply.started":"2024-05-22T16:01:01.114022Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>NER_tags</th>\n","      <th>Tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...</td>\n","      <td>[\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>[O, O, O, O, O, O, O, B-LOC, O, B-LOC, O, O, O...</td>\n","      <td>[Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>[O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...</td>\n","      <td>[\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14357</th>\n","      <td>14358</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Lễ, hội, Trung, thu, năm, nay, diễn, ra, tron...</td>\n","    </tr>\n","    <tr>\n","      <th>14358</th>\n","      <td>14359</td>\n","      <td>[O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...</td>\n","      <td>[Trong, thời, gian, diễn, ra, lễ, hội,, quận, ...</td>\n","    </tr>\n","    <tr>\n","      <th>14359</th>\n","      <td>14360</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[Bên, cạnh, đó,, các, hoạt, động, văn, hóa, co...</td>\n","    </tr>\n","    <tr>\n","      <th>14360</th>\n","      <td>14361</td>\n","      <td>[O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...</td>\n","      <td>[Tại, lễ, khai, mạc,, quận, Hoàn, Kiếm, đã, tr...</td>\n","    </tr>\n","    <tr>\n","      <th>14361</th>\n","      <td>14362</td>\n","      <td>[B-PER, I-PER]</td>\n","      <td>[ĐĂNG, ANH]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14362 rows × 3 columns</p>\n","</div>"],"text/plain":["          Id                                           NER_tags  \\\n","0          1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","1          2  [O, O, O, O, O, O, O, O, O, O, O, B-SYMPTOM_AN...   \n","2          3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","3          4  [O, O, O, O, O, O, O, B-LOC, O, B-LOC, O, O, O...   \n","4          5  [O, O, O, B-PATIENT_ID, O, O, O, O, O, O, O, B...   \n","...      ...                                                ...   \n","14357  14358  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","14358  14359  [O, O, O, O, O, O, O, B-LOC, I-LOC, I-LOC, O, ...   \n","14359  14360  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n","14360  14361  [O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, O, O, ...   \n","14361  14362                                     [B-PER, I-PER]   \n","\n","                                                  Tokens  \n","0      [Đồng, thời, ,, bệnh, viện, tiếp, tục, thực, h...  \n","1      [\", Số, bệnh, viện, có, thể, tiếp, nhận, bệnh,...  \n","2      [Ngoài, ra, ,, những, người, tiếp, xúc, gián, ...  \n","3      [Bà, này, khi, trở, về, quá, cảnh, Doha, (, Qa...  \n","4      [\", Bệnh, nhân, 523, \", và, chồng, là, \", bệnh...  \n","...                                                  ...  \n","14357  [Lễ, hội, Trung, thu, năm, nay, diễn, ra, tron...  \n","14358  [Trong, thời, gian, diễn, ra, lễ, hội,, quận, ...  \n","14359  [Bên, cạnh, đó,, các, hoạt, động, văn, hóa, co...  \n","14360  [Tại, lễ, khai, mạc,, quận, Hoàn, Kiếm, đã, tr...  \n","14361                                        [ĐĂNG, ANH]  \n","\n","[14362 rows x 3 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["combined_train"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.143723Z","iopub.status.busy":"2024-05-22T16:01:01.143418Z","iopub.status.idle":"2024-05-22T16:01:01.148224Z","shell.execute_reply":"2024-05-22T16:01:01.147257Z","shell.execute_reply.started":"2024-05-22T16:01:01.143674Z"},"trusted":true},"outputs":[],"source":["# # Function to convert DataFrame to .conll format\n","# def dataframe_to_conll(df, filepath):\n","#     with open(filepath, 'w', encoding='utf-8') as file:\n","#         for index, row in df.iterrows():\n","#             for token, tag in zip(row['Tokens'], row['NER_tags']):\n","#                 file.write(f\"{token}\\t{tag}\\n\")\n","#             file.write(\"\\n\")  # Separate sentences by a newline\n","            \n","# # Convert DataFrame to .conll file\n","# dataframe_to_conll(combined_train, '/kaggle/working/train.conll')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:01.149539Z","iopub.status.busy":"2024-05-22T16:01:01.149244Z","iopub.status.idle":"2024-05-22T16:01:27.527217Z","shell.execute_reply":"2024-05-22T16:01:27.526234Z","shell.execute_reply.started":"2024-05-22T16:01:01.149490Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['I-AGE',\n"," 'B-GENDER',\n"," 'B-PATIENT_ID',\n"," 'B-TRANSPORTATION',\n"," 'I-MISC',\n"," 'I-SYMPTOM_AND_DISEASE',\n"," 'I-TRANSPORTATION',\n"," 'B-ORG',\n"," 'I-PATIENT_ID',\n"," 'B-MISC',\n"," 'B-SYMPTOM_AND_DISEASE',\n"," 'B-JOB',\n"," 'I-ORG',\n"," 'B-PER',\n"," 'B-LOC',\n"," 'I-LOC',\n"," 'I-PER',\n"," 'B-DATE',\n"," 'B-AGE',\n"," 'I-DATE',\n"," 'I-GENDER',\n"," 'I-JOB',\n"," 'O']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["unique_labels = get_NER_labels(combined_train)\n","unique_labels"]},{"cell_type":"markdown","metadata":{},"source":["# **PhoBERT**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:27.528734Z","iopub.status.busy":"2024-05-22T16:01:27.528421Z","iopub.status.idle":"2024-05-22T16:01:29.250838Z","shell.execute_reply":"2024-05-22T16:01:29.250027Z","shell.execute_reply.started":"2024-05-22T16:01:27.528703Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c91bd0d66604afabc2fb6897355961e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"743f1cbf1aba4c23bece13d383da401d","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40e37bc1a46d4acd94cecee955eac80e","version_major":2,"version_minor":0},"text/plain":["bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34df4ad8816345caa2fbc50081145496","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"]},{"cell_type":"markdown","metadata":{},"source":["# **TẠO LỚP DATASET BẰNG PYTORCH DATASET**"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.252753Z","iopub.status.busy":"2024-05-22T16:01:29.252182Z","iopub.status.idle":"2024-05-22T16:01:29.269202Z","shell.execute_reply":"2024-05-22T16:01:29.268306Z","shell.execute_reply.started":"2024-05-22T16:01:29.252706Z"},"trusted":true},"outputs":[],"source":["def align_label(text, labels, flag=False):\n","    \n","    label_all_tokens = flag #flag xác định cách thực hiện align_label\n","\n","    tokenized_input = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n","    \n","    # Lấy các ID của token từ đầu vào đã được mã hóa\n","    word_ids = tokenized_input.input_ids\n","    \n","    start_part = True # Biến để xác định xem có phải là phần đầu của từ hay không\n","    label_ids = [] # Danh sách để lưu trữ các ID nhãn tương ứng\n","    count = 0 # Biến đếm để theo dõi vị trí trong danh sách labels\n","    \n","    for i in range(len(word_ids)): # Vòng lặp qua từng ID token trong word_ids\n","        \n","        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n","            label_ids.append(-100) # Nếu token là token đặc biệt (0, 1, 2), gán nhãn -100\n","            # Kiểm tra xem token hiện tại có khớp với từ gốc trong văn bản không\n","        elif count < len(text) and ''.join(tokenizer.decode(tokenized_input['input_ids'][i]).split()) == text[count]:\n","            label_ids.append(labels_to_ids[labels[count]]) # Gán nhãn tương ứng từ labels\n","            count+=1 # Tăng biến đếm\n","            start_part = True # Đánh dấu là phần đầu của từ\n","        else:\n","            if start_part:\n","                label_ids.append(labels_to_ids[labels[count]])# Gán nhãn cho phần đầu của từ\n","                count+=1 # Tăng biến đếm\n","                start_part = False # Đánh dấu không còn là phần đầu của từ\n","            else:\n","                label_ids.append(labels_to_ids[labels[count]] if label_all_tokens else -100)\n","                # Nếu không phải phần đầu và flag label_all_tokens là True, gán nhãn cho tất cả các token\n","                # Nếu flag label_all_tokens là False, gán nhãn -100 cho các token không phải phần đầu của từ\n","                \n","    return label_ids\n","\n","class DataSet(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, flag_align_label=False):\n","\n","        lb = df['NER_tags'].values.tolist()\n","        txt = df['Tokens'].values.tolist()\n","        self.texts = [tokenizer(i, padding='max_length', max_length = 256,\n","                                truncation=True, return_tensors=\"pt\", is_split_into_words=True) for i in txt]\n","        self.labels = [align_label(i,j,flag_align_label) for i,j in zip(txt, lb)]\n","        \n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_data(self, idx):\n","        return self.texts[idx]\n","\n","    def get_labels(self, idx):\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","        data = self.get_data(idx)\n","        labels = self.get_labels(idx)\n","\n","        return data, labels"]},{"cell_type":"markdown","metadata":{},"source":["**Tạo 2 dictionary để xác định id nào sẽ là label nào và ngược lại.**"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.270662Z","iopub.status.busy":"2024-05-22T16:01:29.270390Z","iopub.status.idle":"2024-05-22T16:01:29.284956Z","shell.execute_reply":"2024-05-22T16:01:29.284205Z","shell.execute_reply.started":"2024-05-22T16:01:29.270640Z"},"trusted":true},"outputs":[],"source":["labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n","ids_to_labels = {v: k for v, k in enumerate(unique_labels)}"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.286310Z","iopub.status.busy":"2024-05-22T16:01:29.285990Z","iopub.status.idle":"2024-05-22T16:01:29.300749Z","shell.execute_reply":"2024-05-22T16:01:29.299977Z","shell.execute_reply.started":"2024-05-22T16:01:29.286281Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'I-AGE': 0,\n"," 'B-GENDER': 1,\n"," 'B-PATIENT_ID': 2,\n"," 'B-TRANSPORTATION': 3,\n"," 'I-MISC': 4,\n"," 'I-SYMPTOM_AND_DISEASE': 5,\n"," 'I-TRANSPORTATION': 6,\n"," 'B-ORG': 7,\n"," 'I-PATIENT_ID': 8,\n"," 'B-MISC': 9,\n"," 'B-SYMPTOM_AND_DISEASE': 10,\n"," 'B-JOB': 11,\n"," 'I-ORG': 12,\n"," 'B-PER': 13,\n"," 'B-LOC': 14,\n"," 'I-LOC': 15,\n"," 'I-PER': 16,\n"," 'B-DATE': 17,\n"," 'B-AGE': 18,\n"," 'I-DATE': 19,\n"," 'I-GENDER': 20,\n"," 'I-JOB': 21,\n"," 'O': 22}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["labels_to_ids"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.302621Z","iopub.status.busy":"2024-05-22T16:01:29.302065Z","iopub.status.idle":"2024-05-22T16:01:29.312417Z","shell.execute_reply":"2024-05-22T16:01:29.311559Z","shell.execute_reply.started":"2024-05-22T16:01:29.302598Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'I-AGE',\n"," 1: 'B-GENDER',\n"," 2: 'B-PATIENT_ID',\n"," 3: 'B-TRANSPORTATION',\n"," 4: 'I-MISC',\n"," 5: 'I-SYMPTOM_AND_DISEASE',\n"," 6: 'I-TRANSPORTATION',\n"," 7: 'B-ORG',\n"," 8: 'I-PATIENT_ID',\n"," 9: 'B-MISC',\n"," 10: 'B-SYMPTOM_AND_DISEASE',\n"," 11: 'B-JOB',\n"," 12: 'I-ORG',\n"," 13: 'B-PER',\n"," 14: 'B-LOC',\n"," 15: 'I-LOC',\n"," 16: 'I-PER',\n"," 17: 'B-DATE',\n"," 18: 'B-AGE',\n"," 19: 'I-DATE',\n"," 20: 'I-GENDER',\n"," 21: 'I-JOB',\n"," 22: 'O'}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["ids_to_labels"]},{"cell_type":"markdown","metadata":{},"source":["# **BUILD MODEL PHOBERT FOR TOKEN CLASSIFICATION**"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.313880Z","iopub.status.busy":"2024-05-22T16:01:29.313440Z","iopub.status.idle":"2024-05-22T16:01:29.321174Z","shell.execute_reply":"2024-05-22T16:01:29.320465Z","shell.execute_reply.started":"2024-05-22T16:01:29.313857Z"},"trusted":true},"outputs":[],"source":["class PhoBertModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(PhoBertModel, self).__init__()\n","\n","        self.phobert = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(unique_labels))\n","\n","    def forward(self, input_id, mask, label):\n","        \n","        output = self.phobert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# **TRAIN MODEL**"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.322415Z","iopub.status.busy":"2024-05-22T16:01:29.322174Z","iopub.status.idle":"2024-05-22T16:01:29.330148Z","shell.execute_reply":"2024-05-22T16:01:29.329358Z","shell.execute_reply.started":"2024-05-22T16:01:29.322385Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:01:29.331783Z","iopub.status.busy":"2024-05-22T16:01:29.331479Z","iopub.status.idle":"2024-05-22T16:53:02.337800Z","shell.execute_reply":"2024-05-22T16:53:02.336593Z","shell.execute_reply.started":"2024-05-22T16:01:29.331750Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c69483bc425241b7afba3dffb1f127b0","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2024-05-22 16:01:49.359590: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-22 16:01:49.359688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-22 16:01:49.479208: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","100%|██████████| 449/449 [05:36<00:00,  1.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 1 | Loss:  0.256 | Accuracy:  0.937 | Val_Loss:  0.088 | Accuracy:  0.978\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 2 | Loss:  0.057 | Accuracy:  0.985 | Val_Loss:  0.065 | Accuracy:  0.980\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 3 | Loss:  0.032 | Accuracy:  0.991 | Val_Loss:  0.061 | Accuracy:  0.982\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 4 | Loss:  0.022 | Accuracy:  0.994 | Val_Loss:  0.062 | Accuracy:  0.983\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 5 | Loss:  0.017 | Accuracy:  0.995 | Val_Loss:  0.067 | Accuracy:  0.982\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 6 | Loss:  0.014 | Accuracy:  0.996 | Val_Loss:  0.071 | Accuracy:  0.983\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:33<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 7 | Loss:  0.011 | Accuracy:  0.997 | Val_Loss:  0.071 | Accuracy:  0.982\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 449/449 [05:34<00:00,  1.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 8 | Loss:  0.011 | Accuracy:  0.996 | Val_Loss:  0.073 | Accuracy:  0.980\n"]}],"source":["def train_loop(model, train_df, val_df, flag_align_label):\n","\n","    train_dataset = DataSet(train_df, flag_align_label)\n","    val_dataset = DataSet(val_df, flag_align_label)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","        \n","    min_val_loss = 1000\n","    count = 0\n","    \n","    for epoch_num in range(EPOCHS):\n","\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","\n","        for train_data, train_label in tqdm(train_dataloader):\n","\n","            train_label = train_label.to(device)\n","            mask = train_data['attention_mask'].squeeze(1).to(device)\n","            input_id = train_data['input_ids'].squeeze(1).to(device)\n","\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","                logits_clean = logits[i][train_label[i] != -100]\n","                label_clean = train_label[i][train_label[i] != -100]\n","\n","                predictions = logits_clean.argmax(dim=1)\n","                acc = (predictions == label_clean).float().mean()\n","                total_acc_train += acc\n","                total_loss_train += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label.to(device)\n","            mask = val_data['attention_mask'].squeeze(1).to(device)\n","            input_id = val_data['input_ids'].squeeze(1).to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","                logits_clean = logits[i][val_label[i] != -100]\n","                label_clean = val_label[i][val_label[i] != -100]\n","\n","                predictions = logits_clean.argmax(dim=1)\n","                acc = (predictions == label_clean).float().mean()\n","                total_acc_val += acc\n","                total_loss_val += loss.item()\n","\n","        val_accuracy = total_acc_val / len(val_df)\n","        val_loss = total_loss_val / len(val_df)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(train_df): .3f} | Accuracy: {total_acc_train / len(train_df): .3f} | Val_Loss: {total_loss_val / len(val_df): .3f} | Accuracy: {total_acc_val / len(val_df): .3f}')\n","        if val_loss < min_val_loss:\n","            min_val_loss = val_loss\n","            torch.save(model.state_dict(), 'phobert_base_ner')\n","            count = epoch_num\n","        if epoch_num - count >= 5:\n","            return\n","        \n","LEARNING_RATE = 5e-5\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","\n","model = PhoBertModel()\n","train_loop(model, combined_train, combined_val, False)"]},{"cell_type":"markdown","metadata":{},"source":["# **EVALUATE MODEL**"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-22T16:53:02.345319Z","iopub.status.busy":"2024-05-22T16:53:02.344297Z","iopub.status.idle":"2024-05-22T16:53:03.020651Z","shell.execute_reply":"2024-05-22T16:53:03.019396Z","shell.execute_reply.started":"2024-05-22T16:53:02.345287Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'combine_test' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m                 labels\u001b[38;5;241m.\u001b[39mappend([ids_to_labels[val\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m cleaned_labels])\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_pred\u001b[38;5;241m=\u001b[39mpredictions, y_true\u001b[38;5;241m=\u001b[39mlabels, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 32\u001b[0m evaluate(model, \u001b[43mcombine_test\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m, ids_to_labels)\n","\u001b[0;31mNameError\u001b[0m: name 'combine_test' is not defined"]}],"source":["def evaluate(model, test_df, flag_align_label, ids_to_labels):\n","\n","    test_dataset = DataSet(test_df, flag_align_label)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    predictions = []\n","    labels = []\n","    \n","    for test_data, test_label in test_dataloader:\n","\n","            test_label = test_label.to(device)\n","            mask = test_data['attention_mask'].squeeze(1).to(device)\n","\n","            input_id = test_data['input_ids'].squeeze(1).to(device)\n","\n","            loss, logits = model(input_id, mask, test_label)\n","\n","            for i in range(logits.shape[0]):\n","                cleaned_logits = logits[i][test_label[i] != -100].argmax(dim=1)\n","                predictions.append([ids_to_labels[val.item()] for val in cleaned_logits])\n","                cleaned_labels = test_label[i][test_label[i] != -100]\n","                labels.append([ids_to_labels[val.item()] for val in cleaned_labels])\n","    print(classification_report(y_pred=predictions, y_true=labels, digits=3))\n","\n","evaluate(model, combine_test, False, ids_to_labels)"]},{"cell_type":"markdown","metadata":{},"source":["# **INFERENCE**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:53:03.021585Z","iopub.status.idle":"2024-05-22T16:53:03.021946Z","shell.execute_reply":"2024-05-22T16:53:03.021791Z","shell.execute_reply.started":"2024-05-22T16:53:03.021776Z"},"trusted":true},"outputs":[],"source":["def align_word_ids(text, flag):\n","    label_all_tokens = flag\n","    \n","    text = text.split()\n","  \n","    tokenized_inputs = tokenizer(text, padding='max_length', max_length=256, truncation=True, is_split_into_words=True)\n","\n","    word_ids = tokenized_inputs.input_ids\n","\n","    start_part = True\n","    label_ids = []\n","    count = 0\n","    \n","    for i in range(len(word_ids)):\n","        \n","        if word_ids[i] == 0 or word_ids[i] == 1 or word_ids[i] == 2:\n","            label_ids.append(-100)\n","            \n","        elif count < len(text) and ''.join(tokenizer.decode(tokenized_inputs['input_ids'][i]).split()) == text[count]:\n","            label_ids.append(1)\n","            count+=1\n","            start_part = True\n","        else:\n","            if start_part:\n","                label_ids.append(1)\n","                count+=1\n","                start_part = False\n","            else:\n","                label_ids.append(1 if label_all_tokens else -100)           \n","    return label_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:53:03.023347Z","iopub.status.idle":"2024-05-22T16:53:03.023785Z","shell.execute_reply":"2024-05-22T16:53:03.023581Z","shell.execute_reply.started":"2024-05-22T16:53:03.023562Z"},"trusted":true},"outputs":[],"source":["def ner(model, sentence, flag_align_label):\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    if use_cuda:\n","        model = model.cuda()\n","    text = tokenizer(sentence, padding='max_length', max_length = 256, truncation=True, return_tensors=\"pt\")\n","\n","    mask = text['attention_mask'].to(device)\n","    input_id = text['input_ids'].to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence, flag_align_label)).unsqueeze(0).to(device)\n","\n","    logits = model(input_id, mask, None)\n","    logits_clean = logits[0][label_ids != -100]\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    print(sentence)\n","    print(prediction_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-22T16:53:03.024768Z","iopub.status.idle":"2024-05-22T16:53:03.025093Z","shell.execute_reply":"2024-05-22T16:53:03.024949Z","shell.execute_reply.started":"2024-05-22T16:53:03.024935Z"},"trusted":true},"outputs":[],"source":["ner(model,\n","    'Harry Kane là cầu thủ của câu lạc bộ Tottenham nhọ nhất nước Anh',\n","    flag_align_label=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3960915,"sourceId":6895280,"sourceType":"datasetVersion"},{"datasetId":5010229,"sourceId":8416967,"sourceType":"datasetVersion"},{"datasetId":5010243,"sourceId":8416989,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
